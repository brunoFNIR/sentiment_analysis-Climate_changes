{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Dependencies\n",
        "!pip install praw\n",
        "!pip install nltk\n",
        "!pip install transformers\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "id": "sXBi5olem1F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "#get data\n",
        "import praw\n",
        "import webbrowser\n",
        "import os\n",
        "\n",
        "#data manipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "#data pre processing\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from operator import index\n",
        "from transformers import pipeline\n",
        "\n",
        "#model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "\n",
        "#save model and tokenizer\n",
        "from tensorflow.keras.models import load_model\n",
        "import pickle"
      ],
      "metadata": {
        "id": "ZQaYcp17nc14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#credential removed for safety\n",
        "\n",
        "# client_id = \"id\"\n",
        "# client_secret = \"secret\"\n",
        "# user_agent = \"user\"\n",
        "# redirect_uri = \"http://localhost:8080\""
      ],
      "metadata": {
        "id": "41uuIcrZTSN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reddit object\n",
        "reddit = praw.Reddit(\n",
        "    client_id=client_id,\n",
        "    client_secret=client_secret,\n",
        "    user_agent=user_agent,\n",
        "    redirect_uri=redirect_uri\n",
        ")\n",
        "\n",
        "auth_url = reddit.auth.url(scopes=[\"identity\", \"read\"], state=\"random_string\") # \"identity\" para acessar informações do usuário, \"read\" para ler dados públicos\n",
        "print(f\"Abra este URL no seu navegador para autorizar o acesso: {auth_url}\")\n",
        "\n",
        "webbrowser.open(auth_url) # Isso abrirá o URL no seu navegador\n"
      ],
      "metadata": {
        "id": "PepzC_a4TY8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "authorization_code = \"iiQ98BPletKkukTmWJ8H1og4xyOD-w\""
      ],
      "metadata": {
        "id": "w1VY0MmPW0rx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "  reddit.auth.authorize(authorization_code)\n",
        "  print(\"Autorização bem-sucedida!\")\n",
        "except Exception as e:\n",
        "  print(f\"Erro na autorização: {e}\")"
      ],
      "metadata": {
        "id": "4m9PXQLEWeeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subreddit_name = \"climatechange\"\n",
        "search_terms = [\n",
        "    \"climate change\",\n",
        "    \"global warming\",\n",
        "    \"climate crisis\",\n",
        "    \"climate emergency\",\n",
        "    \"environment\",\n",
        "    \"sustainability\",\n",
        "    \"ecology\",\n",
        "    \"climate\",\n",
        "    \"planet\",\n",
        "    \"atmosphere\",\n",
        "    \"greenhouse effect\",\n",
        "    \"carbon emissions\",\n",
        "    \"greenhouse gases\",\n",
        "    \"fossil fuels\",\n",
        "    \"oil\",\n",
        "    \"coal\",\n",
        "    \"natural gas\",\n",
        "    \"deforestation\",\n",
        "    \"wildfires\",\n",
        "    \"agriculture methane emissions\",\n",
        "    \"industrialization\",\n",
        "    \"overconsumption\",\n",
        "    \"extreme weather events\",\n",
        "    \"drought\",\n",
        "    \"flooding\",\n",
        "    \"storm\",\n",
        "    \"hurricane\",\n",
        "    \"cyclone\",\n",
        "    \"heatwave\",\n",
        "    \"forest fires\",\n",
        "    \"sea level rise\",\n",
        "    \"melting glaciers\",\n",
        "    \"biodiversity loss\",\n",
        "    \"species extinction\",\n",
        "    \"ocean acidification\",\n",
        "    \"food security\",\n",
        "    \"water scarcity\",\n",
        "    \"climate migration\",\n",
        "    \"health impacts climate change\",\n",
        "    \"renewable energy\",\n",
        "    \"solar power\",\n",
        "    \"wind power\",\n",
        "    \"hydroelectric power\",\n",
        "    \"geothermal energy\",\n",
        "    \"biomass energy\",\n",
        "    \"energy transition\",\n",
        "    \"decarbonization\",\n",
        "    \"carbon neutrality\",\n",
        "    \"circular economy\",\n",
        "    \"energy efficiency\",\n",
        "    \"carbon capture\",\n",
        "    \"reforestation\",\n",
        "    \"sustainable agriculture\",\n",
        "    \"electric mobility\",\n",
        "    \"green hydrogen\",\n",
        "    \"Paris Agreement\",\n",
        "    \"COP\",\n",
        "    \"COP26\",\n",
        "    \"COP27\",\n",
        "    \"COP28\",\n",
        "    \"COP29\",\n",
        "    \"COP30\",\n",
        "    \"climate policies\",\n",
        "    \"environmental legislation\",\n",
        "    \"climate activism\",\n",
        "    \"climate justice\",\n",
        "    \"climate action\",\n",
        "    \"government climate change\",\n",
        "    \"companies sustainability\",\n",
        "    \"NGOs environment\",\n",
        "    \"IPCC reports\",\n",
        "    \"climate skepticism\",\n",
        "    \"climate denial\",\n",
        "    \"climate misinformation\",\n",
        "    \"climate conspiracy theories\",\n",
        "    \"#ClimateChange\",\n",
        "    \"#GlobalWarming\",\n",
        "    \"#ClimateCrisis\",\n",
        "    \"#ClimateEmergency\",\n",
        "    \"#Environment\",\n",
        "    \"#Sustainability\",\n",
        "    \"#Ecology\",\n",
        "    \"#Climate\",\n",
        "    \"#ActOnClimate\",\n",
        "    \"#SaveOurPlanet\",\n",
        "    \"#GreenNewDeal\",\n",
        "    \"#RenewableEnergy\",\n",
        "    \"#NetZero\",\n",
        "    \"#FridaysForFuture\",\n",
        "    \"#ExtinctionRebellion\",\n",
        "    \"#ClimateJustice\"\n",
        "]\n",
        "limit = 10  # amount limit\n",
        "\n",
        "collected_posts = []\n",
        "\n",
        "try:\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    for term in search_terms:\n",
        "        print(f\"Searching for: '{term}' in r/{subreddit_name}\")\n",
        "        for submission in subreddit.search(query=term, sort=\"relevance\", limit=limit):\n",
        "            post_data = {\n",
        "                \"id\": submission.id,\n",
        "                \"title\": submission.title,\n",
        "                \"author\": str(submission.author),\n",
        "                \"url\": submission.url,\n",
        "                \"created_utc\": submission.created_utc,\n",
        "                \"selftext\": submission.selftext, # post content (if its a text post)\n",
        "                \"subreddit\": subreddit_name,\n",
        "                \"search_term\": term\n",
        "            }\n",
        "            collected_posts.append(post_data)\n",
        "        print(f\"Collected {len(collected_posts)} posts until now.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during collect: {e}\")\n",
        "\n",
        "print(\"\\nCollected data:\")\n",
        "for post in collected_posts:\n",
        "    print(f\"ID: {post['id']}, Título: {post['title']}\")"
      ],
      "metadata": {
        "id": "T5cvE5S2XG8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert dictionary into a dataframe\n",
        "df = pd.DataFrame(collected_posts)"
      ],
      "metadata": {
        "id": "045zQRBvepUi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "A2IaN3DOe4M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "e6bXA0toe7eS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['search_term'].value_counts())"
      ],
      "metadata": {
        "id": "RRezSQ9TgA9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drop duplicates posts\n",
        "df.drop_duplicates(subset='id', keep='first', inplace=True) # Remove duplicados com base no ID único do post"
      ],
      "metadata": {
        "id": "jClJsTi4gLc6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fillna in \"selftext\", avoiding \"combination problems\" --> \"tile\" + \"selftext\"\n",
        "df['selftext'].fillna('', inplace=True)"
      ],
      "metadata": {
        "id": "xTxt08Yogmq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['combined_text_title&selftext'] = df['title'] + '' + df['selftext']"
      ],
      "metadata": {
        "id": "0KlTzdRriATb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['combined_text_title&selftext'] = df['combined_text_title&selftext'].str.lower()"
      ],
      "metadata": {
        "id": "WYlXTYuViXXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing characters different from: numbers, letters or blanks\n",
        "df['combined_text_title&selftext'] = df['combined_text_title&selftext'].apply(lambda x: re.sub(r'[^a-z0-9\\s]]', '', x))"
      ],
      "metadata": {
        "id": "ukyT3InoilGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# words tokenize\n",
        "nltk.download('punkt_tab')\n",
        "df['tokens'] = df['combined_text_title&selftext'].apply(word_tokenize)"
      ],
      "metadata": {
        "id": "-yVnPs1jiwSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing \"stopwords\" (word without significant meaning)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "df['tokens_cleaned'] = df['tokens'].apply(lambda tokens: [word for word in tokens if word not in stop_words])"
      ],
      "metadata": {
        "id": "W8oGWEqikChz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lemmatizing tokens (reducing words into their base form)\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "df['lemmatized_tokens'] = df['tokens_cleaned'].apply(lambda tokens: [lemmatizer.lemmatize(word) for word in tokens])"
      ],
      "metadata": {
        "id": "BtBDJNiwkpDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EMBENDDINGS"
      ],
      "metadata": {
        "id": "BYN8uTj6W9r_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading embeddings files\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip -d embeddings\n",
        "\n",
        "embeddings_index = {}\n",
        "with open('embeddings/glove.6B.100d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype = 'float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f'Found {len(embeddings_index)} word vectors.')"
      ],
      "metadata": {
        "id": "NuoEIJrqlW2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a vocabulary from lematized tokens\n",
        "tokenizer = Tokenizer(num_words = None) # maintain all the words\n",
        "tokenizer.fit_on_texts(df['lemmatized_tokens'].apply(lambda x: ' '.join(x)))\n",
        "word_index = tokenizer.word_index\n",
        "print(f'Vocabulary length: {len(word_index)}')\n",
        "\n",
        "# define max length of sequence for padding\n",
        "MAX_SEQUENCE_LENGTH = max(df['lemmatized_tokens'].apply(len))\n",
        "\n",
        "# create embeddings matrix\n",
        "EMBEDDING_DIM = 100\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # not found words in Glove ll be vectors of zeros\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print(f'Shape of embedding matrix: {embedding_matrix.shape}')"
      ],
      "metadata": {
        "id": "b7bgnv5jY-5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert tokens on number sequences and applying padding (garantee all the sequences to have the same size)\n",
        "sequences = tokenizer.texts_to_sequences(df['lemmatized_tokens'].apply(lambda x: ' '.join(x)))\n",
        "padded_sequences = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
        "\n",
        "print(f'Shape of padded sequences: {padded_sequences.shape}')"
      ],
      "metadata": {
        "id": "H0EENE_Nb5mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model"
      ],
      "metadata": {
        "id": "laPGeoN_ieh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM model\n",
        "VOCAB_SIZE = len(tokenizer.word_index) + 1\n",
        "EMBEDDING_DIM = 100\n",
        "MAX_SEQUENCE_LENGTH = 1124\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH, weights=[embedding_matrix], trainable=False))\n",
        "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "kDswX7U0EoPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Labeling database"
      ],
      "metadata": {
        "id": "HZfwrn5BG05K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")"
      ],
      "metadata": {
        "id": "lipNIOL5HqVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# labeling\n",
        "\n",
        "#loading sentiment-analysis pipeline\n",
        "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "def get_sentiment(text):\n",
        "  try:\n",
        "    # truncating text for max length the model can work with\n",
        "    max_length = sentiment_pipeline.tokenizer.model_max_length - 2\n",
        "    truncated_text = sentiment_pipeline.tokenizer.decode(\n",
        "        sentiment_pipeline.tokenizer.encode(text, truncation=True, max_length=max_length),\n",
        "        skip_special_tokens=True\n",
        "    )\n",
        "    result = sentiment_pipeline(truncated_text)[0]\n",
        "    return result['label'], result['score']\n",
        "  except Exception as e:\n",
        "    print(f\"Error classifying text: {e}\")\n",
        "    return None, None\n",
        "\n",
        "tqdm.pandas()\n",
        "\n",
        "sentiment_results = df['combined_text_title&selftext'].progress_apply(get_sentiment)\n",
        "\n",
        "df[['sentiment_label', 'sentiment_score']]= pd.DataFrame(sentiment_results.tolist(), index=df.index)\n",
        "\n",
        "print(df[['title', 'combined_text_title&selftext', 'sentiment_label', 'sentiment_score']].head())"
      ],
      "metadata": {
        "id": "iFtilYRsIpSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting labels to binarie format\n",
        "sentiment_mapping = {'POSITIVE':1, 'NEGATIVE':0}\n",
        "df['sentiment_label_encoded'] = df['sentiment_label'].map(sentiment_mapping)\n",
        "\n",
        "\n",
        "\n",
        "#getting index of remaining lines\n",
        "# getting index of remaining lines\n",
        "# The issue was here. `df.index` was holding old index values,\n",
        "# even after rows were dropped from the DataFrame.\n",
        "# Resetting the index ensures the index values are in sync with the DataFrame's current state.\n",
        "indexes = df.reset_index(drop=True).index\n",
        "\n",
        "# converting into numpy arrays for Keras\n",
        "X = np.array(padded_sequences)[indexes]\n",
        "y = np.array(df['sentiment_label_encoded'])\n",
        "\n",
        "# splitting data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "print(f'Shape of X_train: {X_train.shape}')\n",
        "print(f'Shape of X_test: {X_test.shape}')\n",
        "print(f'Shape of y_train: {y_train.shape}')\n",
        "print(f'Shape of y_test: {y_test.shape}')"
      ],
      "metadata": {
        "id": "np_CSRUiKFXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Model Training"
      ],
      "metadata": {
        "id": "xa2SnJNirrWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "qmYmcvflqxk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'Loss : {loss:.4f}')\n",
        "print(f'Accuracy: {accuracy:.4f}')"
      ],
      "metadata": {
        "id": "ts3eOXTtvljZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#saving data frame\n",
        "\n",
        "#naming directory\n",
        "data_path = 'data'\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    os.makedirs(data_path)\n",
        "    print(f\"Directory '{data_path}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Directory '{data_path}' already exists.\")\n",
        "\n",
        "#saving\n",
        "df.to_csv('data/df_labeled.csv', index=False)\n",
        "print('DataFrame saved successfully.')"
      ],
      "metadata": {
        "id": "yEhzSuXovmtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving model and tokenizer\n",
        "model_tokenizer_Path = 'model'\n",
        "\n",
        "if not os.path.exists(model_tokenizer_Path):\n",
        "    os.makedirs(model_tokenizer_Path)\n",
        "    print(f\"Directory '{model_tokenizer_Path}' created successfully.\")\n",
        "else:\n",
        "    print(f\"Directory '{model_tokenizer_Path}' already exists.\")\n",
        "\n",
        "#model\n",
        "try:\n",
        "    model.save('model/sentiment_model.keras')\n",
        "    print(\"Model saved successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving the model: {e}\")\n",
        "\n",
        "#tokenizer\n",
        "try:\n",
        "    with open('model/tokenizer.pickle', 'wb') as handle:\n",
        "      pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "      print('Tokenizer saved successfully.')\n",
        "except Exception as e:\n",
        "    print(f\"Error saving the tokenizer: {e}\")"
      ],
      "metadata": {
        "id": "8czOBaHSHyj4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}